{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dc6fd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2d48b4-68b8-4a58-a308-ce33e918dcc4",
   "metadata": {},
   "source": [
    "### Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a7fc615-f9d2-435e-811e-2f14c312e9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "from Bio import Entrez\n",
    "\n",
    "Entrez.email = ''\n",
    "Entrez.api_key = ''\n",
    "\n",
    "def fetch_pubmed_id(query, max_results=100):\n",
    "    handle = Entrez.esearch(db=\"pubmed\", term=query, retmax=max_results)\n",
    "    record = Entrez.read(handle)\n",
    "    handle.close()\n",
    "    return record[\"IdList\"] \n",
    "\n",
    "def fetch_pubmed_abstracts(id_list, batch_size=20):\n",
    "    abstracts = []\n",
    "    for start in range(0, len(id_list), batch_size):\n",
    "        batch_ids = id_list[start:start+batch_size]\n",
    "        ids = \",\".join(batch_ids)\n",
    "        handle = Entrez.efetch(db=\"pubmed\", id=ids, rettype=\"abstract\", retmode=\"xml\")\n",
    "        records = Entrez.read(handle)\n",
    "        handle.close()\n",
    "\n",
    "        for article in records['PubmedArticle']:\n",
    "            article_data = article['MedlineCitation']['Article']\n",
    "            title = article_data.get('ArticleTitle', 'No Title')\n",
    "            abstract_data = article_data.get('Abstract', {}).get('AbstractText', '')\n",
    "            if isinstance(abstract_data, list):\n",
    "                abstract_text = ' '.join([str(a) for a in abstract_data])\n",
    "            elif isinstance(abstract_data, str):\n",
    "                abstract_text = abstract_data\n",
    "            else:\n",
    "                abstract_text = ''\n",
    "\n",
    "            pmid = article['MedlineCitation']['PMID']\n",
    "            mesh_terms = [mesh['DescriptorName'] for mesh in article['MedlineCitation'].get('MeshHeadingList', [])]\n",
    "\n",
    "            abstracts.append({\n",
    "                \"pmid\": str(pmid),\n",
    "                \"title\": str(title),\n",
    "                \"abstract\": str(abstract_text),\n",
    "                \"mesh_terms\": mesh_terms,\n",
    "                \"source\": f\"https://pubmed.ncbi.nlm.nih.gov/{pmid}/\"\n",
    "            })\n",
    "\n",
    "        time.sleep(0.3)  # NCBI rate limits\n",
    "\n",
    "    return abstracts\n",
    "\n",
    "def save_to_json(data, output_file):\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "def fetch_and_save_pubmed_abstracts(query, max_results=100):\n",
    "    for tag, query in SEARCH_QUERIES.items():\n",
    "        ids = fetch_pubmed_id(query, max_results=100)\n",
    "        print(f\"Found {len(ids)} articles.\")\n",
    "\n",
    "        abstracts = fetch_pubmed_abstracts(ids)\n",
    "        # Auto-tagging source type\n",
    "        source_type = \"Bangladesh-specific\" if \"Bangladesh\" in query else \"Global\"\n",
    "        for doc in abstracts:\n",
    "            doc[\"source_type\"] = source_type\n",
    "\n",
    "        output_file = f\"data/processed/{tag}.json\"\n",
    "        save_to_json(abstracts, output_file)\n",
    "        \n",
    "SEARCH_QUERIES = {\n",
    "    # Infectious Diseases (High Priority for Bangladesh)\n",
    "    \"dengue_bangladesh\": \"Dengue AND Bangladesh\",\n",
    "    \"dengue_global\": \"Dengue AND (Treatment OR Guidelines)\",\n",
    "    \"typhoid_bangladesh\": \"Typhoid Fever AND Bangladesh\",\n",
    "    \"typhoid_global\": \"Typhoid Fever AND (Treatment OR Management)\",\n",
    "    \"malaria_bangladesh\": \"Malaria AND Bangladesh\",\n",
    "    \"malaria_global\": \"Malaria AND (Treatment OR Prevention)\",\n",
    "    \"hepatitis_bangladesh\": \"Hepatitis AND Bangladesh\",\n",
    "    \"hepatitis_global\": \"Hepatitis AND (Treatment OR Management)\",\n",
    "    \"diarrhea_bangladesh\": \"Diarrhea AND Bangladesh\",\n",
    "    \"diarrhea_global\": \"Diarrhea AND (Treatment OR Guidelines)\",\n",
    "    \"tuberculosis_bangladesh\": \"Tuberculosis AND Bangladesh\",\n",
    "    \"tuberculosis_global\": \"Tuberculosis AND (Treatment OR WHO Guidelines)\",\n",
    "    \"cholera_bangladesh\": \"Cholera AND Bangladesh\",\n",
    "    \"cholera_global\": \"Cholera AND (Management OR Treatment)\",\n",
    "    \"leptospirosis_bangladesh\": \"Leptospirosis AND Bangladesh\",\n",
    "    \"leptospirosis_global\": \"Leptospirosis AND Treatment\",\n",
    "    \"leishmaniasis_bangladesh\": \"Leishmaniasis AND Bangladesh\",\n",
    "    \"leishmaniasis_global\": \"Leishmaniasis AND Treatment\",\n",
    "    \"influenza_bangladesh\": \"Influenza AND Bangladesh\",\n",
    "    \"influenza_global\": \"Influenza AND Treatment\",\n",
    "\n",
    "    # Non-Communicable Diseases (NCDs)\n",
    "    \"diabetes_bangladesh\": \"Diabetes AND Bangladesh\",\n",
    "    \"diabetes_global\": \"Diabetes AND (Management OR Treatment)\",\n",
    "    \"hypertension_bangladesh\": \"Hypertension AND Bangladesh\",\n",
    "    \"hypertension_global\": \"Hypertension AND Guidelines\",\n",
    "    \"cardiovascular_bangladesh\": \"Cardiovascular Diseases AND Bangladesh\",\n",
    "    \"cardiovascular_global\": \"Cardiovascular Diseases AND Treatment\",\n",
    "    \"ckd_bangladesh\": \"Chronic Kidney Disease AND Bangladesh\",\n",
    "    \"ckd_global\": \"Chronic Kidney Disease AND Management\",\n",
    "    \"cancer_bangladesh\": \"Cancer AND Bangladesh\",\n",
    "    \"cancer_global\": \"Cancer AND (Treatment OR Management)\",\n",
    "\n",
    "    # Maternal & Child Healt\n",
    "    \"maternal_health_bangladesh\": \"Maternal Health AND Bangladesh\",\n",
    "    \"maternal_health_global\": \"Maternal Health AND Guidelines\",\n",
    "    \"neonatal_care_bangladesh\": \"Neonatal Care AND Bangladesh\",\n",
    "    \"neonatal_care_global\": \"Neonatal Care AND WHO Guidelines\",\n",
    "    \"malnutrition_bangladesh\": \"Malnutrition AND Bangladesh\",\n",
    "    \"malnutrition_global\": \"Malnutrition AND Treatment\",\n",
    "    \"immunization_bangladesh\": \"Vaccination AND Bangladesh\",\n",
    "    \"immunization_global\": \"Immunization AND WHO Guidelines\",\n",
    "\n",
    "    # Public Health & Surveillance\n",
    "    \"surveillance_bangladesh\": \"Disease Surveillance AND Bangladesh\",\n",
    "    \"surveillance_global\": \"Disease Surveillance AND WHO\",\n",
    "    \"outbreak_management_bangladesh\": \"Outbreak Response AND Bangladesh\",\n",
    "    \"outbreak_management_global\": \"Outbreak Response AND Guidelines\",\n",
    "    \"health_policy_bangladesh\": \"Health Policy AND Bangladesh\",\n",
    "    \"health_policy_global\": \"Health Policy AND Guidelines\",\n",
    "\n",
    "    # Drug & Treatment Protocols\n",
    "    \"amr_bangladesh\": \"Antibiotic Resistance AND Bangladesh\",\n",
    "    \"amr_global\": \"Antimicrobial Resistance AND WHO Guidelines\",\n",
    "    \"essential_medicines_bangladesh\": \"Essential Medicines AND Bangladesh\",\n",
    "    \"essential_medicines_global\": \"Essential Medicines AND WHO Guidelines\",\n",
    "    \"drug_pricing_bangladesh\": \"Drug Pricing AND Bangladesh\",\n",
    "    \"drug_pricing_global\": \"Drug Pricing AND Policies\",\n",
    "\n",
    "    # General Bangladesh Healthcare Queries\n",
    "    \"healthcare_system_bangladesh\": \"Healthcare System AND Bangladesh\",\n",
    "    \"primary_healthcare_bangladesh\": \"Primary Healthcare AND Bangladesh\",\n",
    "    \"rural_health_services_bangladesh\": \"Rural Health Services AND Bangladesh\",\n",
    "    \"community_health_workers_bangladesh\": \"Community Health Workers AND Bangladesh\",\n",
    "\n",
    "    # General Thematic Searches\n",
    "    \"thematic_infectious_diseases_bd\": \"Infectious Diseases AND Bangladesh\",\n",
    "    \"thematic_ncd_bd\": \"Non-communicable Diseases AND Bangladesh\",\n",
    "    \"thematic_public_health_guidelines_bd\": \"Public Health Guidelines AND Bangladesh\",\n",
    "    \"thematic_disease_surveillance_reports_bd\": \"Bangladesh Disease Surveillance Reports\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec18035e-0441-4ae3-9526-5484b4084878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error extracting text from /data/raw/9789240104907-eng.pdf: [Errno 2] No such file or directory: '/data/raw/9789240104907-eng.pdf'\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "import os\n",
    "\n",
    "path = \"/data/raw/9789240104907-eng.pdf\" # temporary path for testing\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as f:\n",
    "            reader = PyPDF2.PdfReader(f)\n",
    "            for page in reader.pages:\n",
    "                text += page.extract_text() or \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from {pdf_path}: {e}\")\n",
    "    return text\n",
    "\n",
    "def chunk_text(text, chunk_size=1000, overlap=100):\n",
    "    chunks = []\n",
    "    if not text:\n",
    "        return chunks\n",
    "    \n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "def process_pdf_to_chunks(pdf_path, chunk_size=1000, overlap=100):\n",
    "    full_text = extract_text_from_pdf(pdf_path)\n",
    "    return chunk_text(full_text, chunk_size, overlap)\n",
    "\n",
    "\n",
    "print(process_pdf_to_chunks(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "550fb42e-1cbb-45a4-a66b-18a44ecddfac",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scripts'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01muuid\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Import the generic PDF processing function\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscripts\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpdf_to_text\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m process_pdf_to_chunks\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Define directories\u001b[39;00m\n\u001b[32m      9\u001b[39m RAW_PDF_DIR = \u001b[33m\"\u001b[39m\u001b[33mdata/raw/\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'scripts'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import uuid\n",
    "\n",
    "# Import the generic PDF processing function\n",
    "from scripts.preprocessing.pdf_to_text import process_pdf_to_chunks\n",
    "\n",
    "# Define directories\n",
    "RAW_PDF_DIR = \"data/raw/\"\n",
    "PROCESSED_JSON_DIR = \"data/processed\"\n",
    "\n",
    "def main():\n",
    "\n",
    "    os.makedirs(PROCESSED_JSON_DIR, exist_ok=True)\n",
    "    \n",
    "    all_docs = []\n",
    "    for filename in os.listdir(RAW_PDF_DIR):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(RAW_PDF_DIR, filename)\n",
    "            \n",
    "            chunks = process_pdf_to_chunks(pdf_path)\n",
    "            \n",
    "            for i, chunk in enumerate(chunks):\n",
    "                doc_id = str(uuid.uuid4())\n",
    "                all_docs.append({\n",
    "                    \"id\": doc_id,\n",
    "                    \"title\": filename.replace(\".pdf\", \"\"),\n",
    "                    \"body\": chunk,\n",
    "                    \"source\": f\"WHO Guidelines: {filename}\",\n",
    "                    \"language\": \"en\",\n",
    "                    \"source_type\": \"Global\"\n",
    "                })\n",
    "    \n",
    "    output_path = os.path.join(PROCESSED_JSON_DIR, \"who_guidelines.json\")\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(all_docs, f, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"Saved {len(all_docs)} chunks to {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c26a6a18-2696-4ab2-ab22-7d818d1bc8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Updated prompt to be more flexible\n",
    "user_prompt = \"\"\"\n",
    "Extract all available information about medicines from this page. \n",
    "If this is a brand listing page, extract:\n",
    "- List of all brand names\n",
    "- Associated generic names (if available)\n",
    "- Manufacturers (if available)\n",
    "\n",
    "If this is a specific medicine page, extract:\n",
    "- Brand name of the medicine\n",
    "- Generic name\n",
    "- Strength of the medicine\n",
    "- Manufacturer\n",
    "- Dosage form\n",
    "- Indications for use\n",
    "- Pharmacology\n",
    "- Dosage and administration instructions\n",
    "- Precautions or warnings\n",
    "- Side effects\n",
    "- Storage conditions\n",
    "\n",
    "Return the output as a JSON object. If information is not available, use null values.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1c42e521-849f-4bad-b391-b407ec1ee144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping brands page: https://medex.com.bd/brands?page=1\n",
      "Scraping medicine details from: https://medex.com.bd/brands/13717/3-bion-100-mg-tablet\n",
      "Scraping medicine details from: https://medex.com.bd/brands/7695/3-c-200-mg-capsule\n",
      "Scraping medicine details from: https://medex.com.bd/brands/7696/3-c-100-mg-suspension\n",
      "Scraping medicine details from: https://medex.com.bd/brands/18731/3-c-400-mg-capsule\n",
      "Scraping medicine details from: https://medex.com.bd/brands/9538/3-f-500-mg-tablet\n",
      "Scraping medicine details from: https://medex.com.bd/brands/7697/3-geocef-200-mg-capsule\n",
      "Scraping medicine details from: https://medex.com.bd/brands/7698/3-geocef-100-mg-suspension\n",
      "Scraping medicine details from: https://medex.com.bd/brands/31993/3d-20000-iu-capsule\n",
      "Scraping medicine details from: https://medex.com.bd/brands/31994/3d-40000-iu-capsule\n",
      "Scraping medicine details from: https://medex.com.bd/brands/33499/3d-2000-iu-tablet\n",
      "Scraping medicine details from: https://medex.com.bd/brands/7699/3rd-cef-200-mg-tablet\n",
      "Scraping medicine details from: https://medex.com.bd/brands/7700/3rd-cef-400-mg-tablet\n",
      "Scraping medicine details from: https://medex.com.bd/brands/7701/3rd-cef-100-mg-suspension\n",
      "Scraping medicine details from: https://medex.com.bd/brands/17301/5-fu-phares-25-mg-injection\n",
      "Scraping medicine details from: https://medex.com.bd/brands/105/5-fluril-25-mg-injection\n",
      "Scraping medicine details from: https://medex.com.bd/brands/38307/5-in-1-cleanser-cream-cream\n",
      "Scraping medicine details from: https://medex.com.bd/brands/25154/5x-30-mg-tablet\n",
      "Scraping medicine details from: https://medex.com.bd/brands/37637/a-to-z-silver-tablet\n",
      "Scraping medicine details from: https://medex.com.bd/brands/13582/a-b1-100-mg-tablet\n",
      "Scraping medicine details from: https://medex.com.bd/brands/14264/a-cal-500-mg-tablet\n",
      "Scraping medicine details from: https://medex.com.bd/brands/14399/a-cal-d-500-mg-tablet\n",
      "Scraping medicine details from: https://medex.com.bd/brands/14480/a-cal-dx-500-mg-tablet\n",
      "Scraping medicine details from: https://medex.com.bd/brands/15483/a-calm-50-mg-tablet\n",
      "Scraping medicine details from: https://medex.com.bd/brands/3039/a-card-20-mg-tablet\n",
      "Scraping medicine details from: https://medex.com.bd/brands/14006/a-care-6-mg-tablet\n",
      "Scraping medicine details from: https://medex.com.bd/brands/6171/a-clox-500-mg-capsule\n",
      "Scraping medicine details from: https://medex.com.bd/brands/6172/a-clox-125-mg-suspension\n",
      "Scraping medicine details from: https://medex.com.bd/brands/6173/a-clox-250-mg-injection\n",
      "Scraping medicine details from: https://medex.com.bd/brands/6174/a-clox-500-mg-injection\n",
      "Scraping medicine details from: https://medex.com.bd/brands/4006/a-cof-10-mg-syrup\n",
      "Finished crawling all available pages or reached the page limit.\n",
      "Successfully scraped 30 medicine brands and saved to medex_data.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Base URL for the Medex website\n",
    "BASE_URL = 'https://medex.com.bd'\n",
    "BRANDS_URL = f'{BASE_URL}/brands?page='\n",
    "\n",
    "# List to hold all the scraped medicine data\n",
    "all_medicines = []\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Removes leading/trailing whitespace and cleans up multiple spaces.\"\"\"\n",
    "    if text:\n",
    "        return ' '.join(text.strip().split())\n",
    "    return None\n",
    "\n",
    "def find_section_content(soup, heading_text):\n",
    "    # Find the heading element (h3, h4 or h5) containing the heading text\n",
    "    heading = soup.find(['h3', 'h4', 'h5'], string=lambda t: t and heading_text in t)\n",
    "    \n",
    "    if heading:\n",
    "        parent_div = heading.parent\n",
    "        content_div = parent_div.find_next_sibling()\n",
    "\n",
    "        if content_div and 'ac-body' in content_div.get('class', []):\n",
    "            # Use get_text() to retrieve all text nodes, including nested ones\n",
    "            return clean_text(content_div.get_text(separator=' ', strip=True))\n",
    "    return None\n",
    "\n",
    "def extract_medicine_details(page_url):\n",
    "    \"\"\"\n",
    "    Scrapes a single medicine brand page and returns a dictionary of its details.\n",
    "    \n",
    "    Args:\n",
    "        page_url (str): The URL of the specific medicine page.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary containing the medicine's details, or None if scraping fails.\n",
    "    \"\"\"\n",
    "    print(f\"Scraping medicine details from: {page_url}\")\n",
    "    try:\n",
    "        response = requests.get(page_url, timeout=10)\n",
    "        response.raise_for_status()  # Raises an HTTPError for bad responses (4xx or 5xx)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Create a dictionary to store the scraped data\n",
    "        medicine_item = {}\n",
    "        \n",
    "        # --- Extract key information ---\n",
    "        \n",
    "        # Brand Name\n",
    "        brand_name = soup.select_one('h1.page-heading-1-l')\n",
    "        medicine_item['brand_name'] = clean_text(brand_name.text) if brand_name else None\n",
    "\n",
    "        # Generic Name and Link\n",
    "        generic_info = soup.select_one('div[title=\"Generic Name\"] a')\n",
    "        if generic_info:\n",
    "            medicine_item['generic_name'] = clean_text(generic_info.text)\n",
    "        else:\n",
    "            medicine_item['generic_name'] = None\n",
    "           \n",
    "\n",
    "        # Manufacturer Name\n",
    "        # Corrected selector to match the HTML in the user's example\n",
    "        manufacturer_info = soup.find('div', title='Manufactured by')\n",
    "        if manufacturer_info:\n",
    "            manufacturer_name_tag = manufacturer_info.find('a')\n",
    "            medicine_item['manufacturer_name'] = clean_text(manufacturer_name_tag.text) if manufacturer_name_tag else None\n",
    "        else:\n",
    "            medicine_item['manufacturer_name'] = None\n",
    "\n",
    "        # Dosage Form\n",
    "        # Corrected selector to find the <small> tag inside the main heading\n",
    "        dosage_form_tag = soup.select_one('h1.page-heading-1-l small[title=\"Dosage Form\"]')\n",
    "        if dosage_form_tag:\n",
    "            medicine_item['dosage_form'] = clean_text(dosage_form_tag.text)\n",
    "        else:\n",
    "            medicine_item['dosage_form'] = None\n",
    "\n",
    "\n",
    "        # Strength\n",
    "        strength = soup.select_one('div[title=\"Strength\"]')\n",
    "        medicine_item['strength'] = clean_text(strength.text) if strength else None\n",
    "        \n",
    "        # Unit Price\n",
    "        # Corrected selector to find the unit price within the package-container\n",
    "        price_info = soup.find('div', class_='package-container')\n",
    "        if price_info:\n",
    "            # The unit price is the second <span> tag within the package container\n",
    "            price_span = price_info.find_all('span')\n",
    "            if len(price_span) > 1:\n",
    "                medicine_item['unit_price'] = clean_text(price_span[1].text)\n",
    "            else:\n",
    "                medicine_item['unit_price'] = None\n",
    "        else:\n",
    "            medicine_item['unit_price'] = None\n",
    "        \n",
    "        # --- Extract detailed sections using the new helper function ---\n",
    "        medicine_item['indications'] = find_section_content(soup, 'Indications')\n",
    "        medicine_item['pharmacology'] = find_section_content(soup, 'Pharmacology')\n",
    "        medicine_item['dosage_and_administration'] = find_section_content(soup, 'Dosage & Administration')\n",
    "        medicine_item['contraindications'] = find_section_content(soup, 'Contraindications')\n",
    "        medicine_item['side_effects'] = find_section_content(soup, 'Side Effects')\n",
    "        medicine_item['pregnancy_and_lactation'] = find_section_content(soup, 'Pregnancy & Lactation')\n",
    "        medicine_item['precautions_and_warnings'] = find_section_content(soup, 'Precautions & Warnings')\n",
    "        medicine_item['overdose_effects'] = find_section_content(soup, 'Overdose Effects')\n",
    "            \n",
    "        return medicine_item\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {page_url}: {e}\")\n",
    "        return None\n",
    "    except AttributeError:\n",
    "        print(f\"Could not find all data on page: {page_url}. Skipping.\")\n",
    "        return None\n",
    "\n",
    "def scrape_brand_page(page_number):\n",
    "    \"\"\"\n",
    "    Scrapes a single page of medicine brands and extracts the links to each medicine.\n",
    "    \n",
    "    Args:\n",
    "        page_number (int): The page number to scrape.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of URLs for the individual medicine brand pages.\n",
    "    \"\"\"\n",
    "    url = f\"{BRANDS_URL}{page_number}\"\n",
    "    print(f\"Scraping brands page: {url}\")\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all medicine links on the page\n",
    "        medicine_links = soup.select('a.hoverable-block')\n",
    "        \n",
    "        # Extract the href attribute from each link. The URLs are already absolute.\n",
    "        links = [link.get('href') for link in medicine_links]\n",
    "        \n",
    "        # Check for pagination to see if there's a next page\n",
    "        next_page_link = soup.select_one('a.page-link[rel=\"next\"]')\n",
    "        has_next_page = next_page_link is not None\n",
    "        \n",
    "        return links, has_next_page\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching brands page {page_number}: {e}\")\n",
    "        return [], False\n",
    "\n",
    "def main_crawler(start_page=1, max_pages=None):\n",
    "    \"\"\"\n",
    "    The main function to orchestrate the crawling process.\n",
    "    \n",
    "    Args:\n",
    "        start_page (int): The page number to start crawling from.\n",
    "        max_pages (int, optional): The maximum number of pages to crawl. Crawls all pages if None.\n",
    "    \"\"\"\n",
    "    page_number = start_page\n",
    "    while True:\n",
    "        # Scrape the current brands page\n",
    "        medicine_urls, has_next_page = scrape_brand_page(page_number)\n",
    "        \n",
    "        # If no links are found, stop crawling\n",
    "        if not medicine_urls:\n",
    "            print(\"No more pages to crawl or an error occurred.\")\n",
    "            break\n",
    "        \n",
    "        # Iterate through the extracted medicine URLs and get detailed info\n",
    "        for url in medicine_urls:\n",
    "            details = extract_medicine_details(url)\n",
    "            if details:\n",
    "                all_medicines.append(details)\n",
    "            time.sleep(1)  # Be polite to the server with a short delay\n",
    "        \n",
    "        # Check if we should stop\n",
    "        if not has_next_page or (max_pages and page_number >= max_pages):\n",
    "            print(\"Finished crawling all available pages or reached the page limit.\")\n",
    "            break\n",
    "            \n",
    "        page_number += 1\n",
    "        time.sleep(2)  # Longer delay between pages to be more respectful\n",
    "\n",
    "    # Save the scraped data to a JSON file\n",
    "    with open('medex_data.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_medicines, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "    print(f\"Successfully scraped {len(all_medicines)} medicine brands and saved to medex_data.json\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # You can set a max_pages limit for testing purposes, e.g., max_pages=5\n",
    "    main_crawler(max_pages=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5a00a5-859c-4352-9629-475c1418aa83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
